---
title: "Competition"
author: "Fadi Hamad"
date: "3/18/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages and data

```{r, warning=FALSE, message=FALSE, echo=FALSE}
library(caret)
library(lattice)
library(pls)
library(tidyverse)
library(elasticnet)
library(corrplot)
library(ggplot2)
#library(Metrics)
library(readr)
```

```{r, warning=FALSE, message=FALSE, echo=FALSE}
raw_data <- read_csv("competition-data.csv")
head(raw_data)
```

# Reviewing the Data
## Density Plot

```{r, warning=FALSE, message=FALSE, echo=FALSE}
density_plot <- ggplot(raw_data, aes(x=outcome)) + 
  geom_density()
density_plot
```
## Histogram 
```{r}
plot.histogram <- raw_data %>%
  ggplot(aes(outcome))+
  geom_histogram(bins = 20, color = "black", fill = "blue", alpha = .4)
plot.histogram
```
## predictor varaition 
```{r}
predictor.var <- raw_data %>%
  select(-outcome) %>%
  summarise_all(funs(sd)) 
#predictor.var

low.var <- predictor.var %>%
  select_if(colSums(predictor.var)<10)
low.var

only.high.var.predictors <- raw_data %>%
  select(-X1,-X6,-X7,-X14)
```
  + Since predictors with low variance have less predictive value and con over fit models (esppecially nueral networks), we could consider dropping the 4 predictors with the least variation.
  
## Percent of 0 outcomes
```{r}
num.zero <- raw_data %>%
  filter(outcome==0) %>%
  summarise(count = n())

per.zero <- num.zero/nrow(raw_data)

data.frame(Count.Zero = num.zero,
           Percent.Zero = per.zero)
```
  + Could try dropping rows with outcomes of 0 to improve predictions of non-zero. Then predict 3.5% of outcomes as 0 artificially. My not work, I just remember using this technique in my simulations class. 
  
# PreProccing the data

## Investigate the missing values

```{r, warning=FALSE, message=FALSE, echo=FALSE}
Total.Missing <- raw_data%>%
  summarise(sum(is.na(across(everything()))))
Total.Missing

head(raw_data %>% filter(is.na(outcome)))
```

## Removing all rows with NAs
  
```{r, warning=FALSE, message=FALSE, echo=FALSE}
# remove all rows with NAs
drop_na_raw_data <- raw_data %>% drop_na()
```

## Batch transformation

```{r, warning=FALSE, message=FALSE, echo=FALSE}
predictors_only <- select(drop_na_raw_data, -outcome)
# Change predictors_only from tible to data.frame as 
# preProcess only accepts dataframes
df_predictors_only <- data.frame(predictors_only) 
modifiedData_preprocessing_fit <- preProcess(
  df_predictors_only, 
  method = c("BoxCox", "center", "scale"))
modifiedData_preprocessing_fit
```

## The dataset after preprocessing

```{r, warning=FALSE, message=FALSE, echo=FALSE}
transformed_predictors <- predict(
  modifiedData_preprocessing_fit, df_predictors_only)

preprocessed_data <- add_column(transformed_predictors, 
     outcome = drop_na_raw_data$outcome)

head(preprocessed_data)
```

## Set up cross validation

```{r, warning=FALSE, message=FALSE, echo=FALSE}
set.seed(100)
number_train <- floor(0.7 * nrow(preprocessed_data))
training_indicies <- sample(1:nrow(preprocessed_data), number_train)
train <- preprocessed_data[training_indicies,]
validate <- preprocessed_data[-training_indicies,]
validate_predictors <- validate %>%
  select(-outcome)

folds <- createFolds(train$outcome, 
                     k = 5, 
                     returnTrain = TRUE)
ctrl <- trainControl(method = "cv",
                     index = folds)
```

## Remove highly correlated predictors

```{r, warning=FALSE, message=FALSE, echo=FALSE}
tooHigh <- findCorrelation(cor(train), cutoff = .9)
trainXnnet <- train[, -tooHigh]
validatennet <- validate[, -tooHigh]

training_data_predicators <- trainXnnet %>% select(-outcome)
training_data_outcome <- trainXnnet %>% select(outcome)
```

# Model Buidling

## One Layer Neural Network Model

```{r, warning=FALSE, message=FALSE, echo=FALSE}
nnetGrid <- expand.grid(
  decay = c(5.0, 20.0, 80.0),
  size = c(1, 2, 3)
)                        
set.seed(100)
nnetTuneWeightDecay <- train(training_data_predicators, training_data_outcome$outcome,
                             method = "nnet",
                             tuneGrid = nnetGrid,
                             trControl = ctrl,
                             linout = TRUE,
                             trace = FALSE
                             )
```


```{r, warning=FALSE, message=FALSE, echo=FALSE}
min(nnetTuneWeightDecay$results$RMSE)
ggplot(nnetTuneWeightDecay)
```
  + Neural Network Predictions and Test RMSE
```{r}
nnet.predictions <- predict(nnetTuneWeightDecay, validatennet)
nnnet.test.rmse <- RMSE(nnet.predictions, validate$outcome)
print(paste0("Test RMSE: ",nnnet.test.rmse))
```

## Tree Based Models 

```{r}

rf_gid <- data.frame(mtry = c(2,4,6,8,10,12))

rand_tree_model <- train(x = training_data_predicators,
                         y = training_data_outcome$outcome,
                         tuneGrid = rf_gid,
                         method = "rf", 
                         ntree = 50,
                         importance = TRUE,
                         trControl = ctrl)
```

  + Models vs the  training RMSE
```{r}
ggplot(rand_tree_model)+
  labs(title = "K vs. RMSE")
```
  
 + Best Random Forest Model
```{r}
rf.min.rmse<-min(rand_tree_model$results$RMSE)

as.data.frame(rand_tree_model$results) %>%
  filter(RMSE == rf.min.rmse) %>%
  select(mtry,RMSE)

print(paste0("Min Train RMSE: ",rf.min.rmse))
```
  + Predictions and test RMSE
```{r}
rf.predictions <- predict(rand_tree_model, validatennet)
rf.test.rmse <- RMSE(rf.predictions, validate$outcome)
print(paste0("Test RMSE: ",rf.test.rmse))
```

## Support Vector Machine
  + The smv model
```{r}
svm.grid <- expand.grid(.sigma = c(.001,.01,.1,1,10), .C = c(5,10,15,20))
svmRTuned <- train(x = training_data_predicators,
                   y = training_data_outcome$outcome,
                   method = "svmRadial",
                   preProc = c("center", "scale"),
                   #tuneLength = 6,
                   tuneGrid = svm.grid,
                   trControl = ctrl)
                   
```
  
  + Best svm model
```{r}
min.svm.rmse <- min(svmRTuned$results$RMSE)

best.svm.rmse <- as.data.frame(svmRTuned$result) %>%
  filter(min.svm.rmse==RMSE) %>%
  select(sigma,C,RMSE)

best.svm.rmse
```
 
  + RMSE vs. Sigma and Cost
```{r}
ggplot(svmRTuned)+
  scale_x_log10()+
  labs(title = "Model vs. RMSE")
```

  + Predictions 
 - can not get the svm function to make predictions -
 - but the train RMSE is already high so it probably a bad choice - 
```{r}

#smv.predictions <- predict(object = svmRTuned, newdata=validatennet)

#svm.test.rmse <- RMSE(smv.predictions, validate)
#svm.test.rmse

```

## Multivariate Adaptive Regression Spline

```{r,warning=FALSE, error=FALSE, message=FALSE}

mars_grid <- expand.grid(.degree = c(1,2),
                         .nprune = c(7,10,13,16,19,26))

mars_model <- train(x = training_data_predicators,
                   y = training_data_outcome$outcome,
                   method = "earth",
                   tuneGrid = mars_grid,
                   trControl = ctrl)
```

  + Models vs the training RMSE
```{r}
ggplot(mars_model)+
  labs(title = "Degree and Terms vs. RMSE")
```
  
 + Best MARS Model
```{r}
mars.min.rmse<-min(mars_model$results$RMSE)

as.data.frame(mars_model$results)%>%
  filter(RMSE == mars.min.rmse) %>%
  select(degree,nprune,RMSE)

print(paste0("Min Train RMSE: ",mars.min.rmse))
```
 
  + Predictions and test RMSE
```{r}
mars.predictions <- predict(mars_model, validatennet)
mars.test.rmse <- RMSE(mars.predictions, validate$outcome)
print(paste0("Test RMSE: ",mars.test.rmse))
```

